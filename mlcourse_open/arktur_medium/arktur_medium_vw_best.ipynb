{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from datetime import datetime\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH_TO_DATA = '../../data/arktur_medium/'\n",
    "!PATH_TO_DATA=../../data/arktur_medium/\n",
    "\n",
    "INP_TRAIN = \"train.json\"\n",
    "INP_TEST  = \"test.json\"\n",
    "!INP_TRAIN=train.json\n",
    "!INP_TEST=test.json\n",
    "\n",
    "OUT_TRAIN = \"train.vw\"\n",
    "OUT_PART_TRAIN = \"part_train.vw\"\n",
    "OUT_PART_VALID = \"part_valid.vw\"\n",
    "OUT_TEST = \"test.vw\"\n",
    "!OUT_TRAIN=train.vw\n",
    "!OUT_PART_TRAIN=part_train.vw\n",
    "!OUT_PART_VALID=part_valid.vw\n",
    "!OUT_TEST=test.vw\n",
    "\n",
    "LOG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove html tags\n",
    "\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ' '.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_target = pd.read_csv(os.path.join(PATH_TO_DATA, \n",
    "                                        'train_log1p_recommends.csv'), \n",
    "                           index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform data to vw format\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lmtzr = WordNetLemmatizer()\n",
    "stop = set(stopwords.words(\"english\") + list(string.punctuation) + [\"’\", \"“\", \"”\"])\n",
    "tokenize = lambda s: \" \".join(lmtzr.lemmatize(i) for i in word_tokenize(s.lower()) if i not in stop)\n",
    "\n",
    "\n",
    "def to_vw(data, label):\n",
    "    exclude = set(string.punctuation)\n",
    "       \n",
    "    content = strip_tags(data[\"content\"]).replace(\"\\n\", \" \").replace(\":\", \"\").replace(\"|\", \"\")\n",
    "    content = tokenize(content)\n",
    "    \n",
    "    title = strip_tags(data[\"title\"]).replace(\"\\n\", \" \").replace(\":\", \"\").replace(\"|\", \"\")\n",
    "    title = tokenize(title)\n",
    "    \n",
    "    author = strip_tags(data[\"meta_tags\"][\"author\"]).replace(\"\\n\", \" \").replace(\":\", \"\").replace(\"|\", \"\")\n",
    "    author = \"\".join(ch for ch in author if ch not in exclude).replace(\" \", \"_\").lower()\n",
    "    \n",
    "    domain = data[\"domain\"].replace(\"\\n\", \" \").replace(\":\", \"\").replace(\"|\", \"\").lower()\n",
    "    \n",
    "    # date time\n",
    "    dt = datetime.strptime(data[\"published\"][\"$date\"][:-1], \"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "    month = dt.month\n",
    "    weekday = dt.weekday()\n",
    "    hour = dt.hour \n",
    "    read_time = data[\"meta_tags\"][\"twitter:data1\"].split(\" \")[0]\n",
    "    read_time = read_time if read_time else 0.1\n",
    "    \n",
    "    # binary\n",
    "    is_weekend = 1 if weekday in (5, 6) else 0\n",
    "    is_night = 1 if hour in range(0, 7) else 0\n",
    "    is_morning = 1 if hour in range(7, 12) else 0\n",
    "    is_noon = 1 if hour in range(12, 19) else 0\n",
    "    is_eve = 1 if hour in range(19, 24) else 0\n",
    "    is_image = 1 if data[\"image_url\"] else 0\n",
    "\n",
    "\n",
    "    out = [\n",
    "        str(np.log(label)),\n",
    "        \"a_content %s\" % content,\n",
    "        \"b_author %s\" % author,\n",
    "        \"c_title %s\" % title,\n",
    "        \"d_domain %s\" % domain,\n",
    "        \"e_num 10:%s 11:%s 12:%s 13:%s\" % (month, weekday + 1, hour + 1, read_time),\n",
    "        \"f_binary 0:%s 1:%s 2:%s 3:%s 4:%s 5:%s\" % \n",
    "        (is_weekend, is_night, is_morning, is_noon, is_eve, is_image)\n",
    "    ]\n",
    "    \n",
    "    return str(\" |\".join(out).encode('ascii', 'ignore').strip())[2:-1] + \"\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b7fed40171f431c8f7229682200b7a4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "\n",
    "with codecs.open(os.path.join(PATH_TO_DATA, INP_TRAIN), encoding=\"utf-8\") as inp_json, \\\n",
    "     codecs.open(os.path.join(PATH_TO_DATA, OUT_TRAIN), 'w', encoding=\"utf-8\") as out_train:\n",
    "    N = 52699\n",
    "#     N = 100        \n",
    "    header = False\n",
    "    \n",
    "    for n, json_ in enumerate(tqdm_notebook(inp_json, total=N)):\n",
    "        if n == N: break\n",
    "\n",
    "        json_data = json.loads(json_)\n",
    "        label = train_target.iloc[n][\"log_recommends\"]\n",
    "        out = to_vw(json_data, label)\n",
    "        out_train.write(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split train into part_train and part_valid\n",
    "\n",
    "!split -l 36889 $PATH_TO_DATA/$OUT_TRAIN $PATH_TO_DATA/$OUT_TRAIN\"_\"\n",
    "\n",
    "!mv $PATH_TO_DATA/$OUT_TRAIN\"_aa\" $PATH_TO_DATA/$OUT_PART_TRAIN\n",
    "!mv $PATH_TO_DATA/$OUT_TRAIN\"_ab\" $PATH_TO_DATA/$OUT_PART_VALID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from validation target file \n",
    "\n",
    "!cut -f 1 -d ' ' $PATH_TO_DATA/$OUT_PART_VALID > $PATH_TO_DATA/part_valid_target.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 2-grams for all namespaces.\n",
      "using l2 regularization = 1e-05\n",
      "final_regressor = ../../data/arktur_medium//model.vw\n",
      "Num weight bits = 24\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = ../../data/arktur_medium//medium.cache\n",
      "Reading datafile = ../../data/arktur_medium//part_train.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "3.735672 3.735672            1            1.0   1.9328   0.0000     1235\n",
      "2.101581 0.467491            2            2.0   1.7891   1.1053      907\n",
      "1.536927 0.972273            4            4.0   0.8340   0.2407      485\n",
      "0.921641 0.306355            8            8.0   1.6958   1.9328     4389\n",
      "0.789018 0.656394           16           16.0   0.3266   0.3900      799\n",
      "0.559749 0.330479           32           32.0   1.3425   1.8813     1607\n",
      "0.454237 0.348724           64           64.0   0.0940   0.2947      219\n",
      "0.444810 0.435384          128          128.0   1.5040   0.9506     1691\n",
      "0.371474 0.298137          256          256.0   0.9962   0.4049     2921\n",
      "0.316328 0.261182          512          512.0   0.0940   0.2670      607\n",
      "0.356122 0.395917         1024         1024.0   1.7346   2.1478     2999\n",
      "0.352597 0.349072         2048         2048.0   1.2429   1.3128     1097\n",
      "0.333247 0.313897         4096         4096.0   0.7321   0.4850     1555\n",
      "0.312224 0.291202         8192         8192.0   0.8340   0.7399      805\n",
      "0.294992 0.277759        16384        16384.0   1.4215   1.6414     2653\n",
      "0.278108 0.261224        32768        32768.0   1.3589   0.2250      121\n",
      "0.246902 0.246902        65536        65536.0   0.6657   0.7381      133 h\n",
      "0.234884 0.222866       131072       131072.0  -0.3665  -0.3665     1059 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 33201\n",
      "passes used = 5\n",
      "weighted example sum = 166005.000000\n",
      "weighted label sum = 101622.523397\n",
      "average loss = 0.221504 h\n",
      "best constant = 0.612165\n",
      "total feature number = 198668435\n",
      "CPU times: user 1.42 s, sys: 208 ms, total: 1.63 s\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!vw -d $PATH_TO_DATA/$OUT_PART_TRAIN -k --cache_file=$PATH_TO_DATA/medium.cache \\\n",
    "  --loss_function squared --ngram=2 --passes 5 -b 24 -l 0.5 --power_t=0.5 --l2=1e-5 \\\n",
    "  -f $PATH_TO_DATA/model.vw #--readable_model=$PATH_TO_DATA/readable_model.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41619083061151058"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict for validation and MAE\n",
    "\n",
    "!vw -i $PATH_TO_DATA/model.vw -t -d $PATH_TO_DATA/$OUT_PART_VALID \\\n",
    "-p $PATH_TO_DATA/part_valid_predictions.txt --quiet\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "y_true = np.loadtxt(PATH_TO_DATA + '/part_valid_target.txt')\n",
    "y_pred = np.loadtxt(PATH_TO_DATA + '/part_valid_predictions.txt')\n",
    "\n",
    "mean_absolute_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 2-grams for all namespaces.\n",
      "using l2 regularization = 1e-05\n",
      "final_regressor = ../../data/arktur_medium//model.vw\n",
      "Num weight bits = 24\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = ../../data/arktur_medium//medium.cache\n",
      "Reading datafile = ../../data/arktur_medium//train.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "3.735672 3.735672            1            1.0   1.9328   0.0000     1235\n",
      "2.101581 0.467491            2            2.0   1.7891   1.1053      907\n",
      "1.536927 0.972273            4            4.0   0.8340   0.2407      485\n",
      "0.921641 0.306355            8            8.0   1.6958   1.9328     4389\n",
      "0.789018 0.656394           16           16.0   0.3266   0.3900      799\n",
      "0.559749 0.330479           32           32.0   1.3425   1.8813     1607\n",
      "0.454237 0.348724           64           64.0   0.0940   0.2947      219\n",
      "0.444810 0.435384          128          128.0   1.5040   0.9506     1691\n",
      "0.371474 0.298137          256          256.0   0.9962   0.4049     2921\n",
      "0.316328 0.261182          512          512.0   0.0940   0.2670      607\n",
      "0.356122 0.395917         1024         1024.0   1.7346   2.1478     2999\n",
      "0.352597 0.349072         2048         2048.0   1.2429   1.3128     1097\n",
      "0.333247 0.313897         4096         4096.0   0.7321   0.4850     1555\n",
      "0.312224 0.291202         8192         8192.0   0.8340   0.7399      805\n",
      "0.294992 0.277759        16384        16384.0   1.4215   1.6414     2653\n",
      "0.278108 0.261224        32768        32768.0   1.3589   0.2250      121\n",
      "0.246704 0.246704        65536        65536.0   0.9102   1.8794     3607 h\n",
      "0.232464 0.218226       131072       131072.0   1.4013   1.3919      337 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 47430\n",
      "passes used = 5\n",
      "weighted example sum = 237150.000000\n",
      "weighted label sum = 145199.925622\n",
      "average loss = 0.212029 h\n",
      "best constant = 0.612270\n",
      "total feature number = 279159360\n",
      "CPU times: user 1.8 s, sys: 200 ms, total: 2 s\n",
      "Wall time: 1min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!vw -d $PATH_TO_DATA/$OUT_TRAIN -k --cache_file=$PATH_TO_DATA/medium.cache \\\n",
    "  --loss_function squared --ngram=2 --passes 5 -b 24 -l 0.5 --power_t=0.5 --l2=1e-5 \\\n",
    "  -f $PATH_TO_DATA/model.vw #--readable_model=$PATH_TO_DATA/readable_model.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d2996aae97a4cc497b5bab8c5c77eb8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# create test data\n",
    "\n",
    "import codecs\n",
    "\n",
    "with codecs.open(os.path.join(PATH_TO_DATA, INP_TEST), encoding=\"utf-8\") as inp_json, \\\n",
    "     codecs.open(os.path.join(PATH_TO_DATA, OUT_TEST), 'w', encoding=\"utf-8\") as out_test:\n",
    "    N = 39492        \n",
    "\n",
    "    for n, json_ in enumerate(tqdm_notebook(inp_json, total=N)):\n",
    "        if n == N: break\n",
    "\n",
    "        json_data = json.loads(json_)\n",
    "\n",
    "        out = to_vw(json_data, 1)\n",
    "        out_test.write(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict for test\n",
    "\n",
    "!vw -i $PATH_TO_DATA/model.vw -t -d $PATH_TO_DATA/test.vw \\\n",
    "-p $PATH_TO_DATA/test_predictions.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create submission\n",
    "\n",
    "submission = pd.read_csv(PATH_TO_DATA + '/sample_submission.csv', index_col='id')\n",
    "if LOG:\n",
    "    submission['log_recommends'] = np.exp(np.loadtxt(PATH_TO_DATA + '/test_predictions.txt'))\n",
    "else:\n",
    "    submission['log_recommends'] = np.loadtxt(PATH_TO_DATA + '/test_predictions.txt')\n",
    "submission.to_csv(PATH_TO_DATA + '/submit/best_n2_p5_b24_l05_p05_l2_1e5.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
