{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH_TO_DATA = \"../../data/kaggle_alice/\"\n",
    "!PATH_TO_DATA=../../data/kaggle_alice/\n",
    "\n",
    "INP_TRAIN = \"train_sessions.csv\"\n",
    "INP_TEST  = \"test_sessions.csv\"\n",
    "SITE_DIC = \"site_dic.pkl\"\n",
    "SAMPLE_SUBMIT = \"sample_submission.csv\"\n",
    "\n",
    "!INP_TRAIN=train_sessions.csv\n",
    "!INP_TEST=test_sessions.csv\n",
    "!SITE_DIC=site_dic.pkl\n",
    "!SAMPLE_SUBMIT=sample_submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_auc_lr_valid(X, y, C=1.0, ratio=0.7, seed=17):\n",
    "    '''\n",
    "    X, y – выборка\n",
    "    ratio – в каком отношении поделить выборку\n",
    "    C, seed – коэф-т регуляризации и random_state \n",
    "              логистической регрессии\n",
    "    '''\n",
    "    train_len = int(ratio * X.shape[0])\n",
    "    X_train = X[:train_len, :]\n",
    "    X_valid = X[train_len:, :]\n",
    "    y_train = y[:train_len]\n",
    "    y_valid = y[train_len:]\n",
    "    \n",
    "    logit = LogisticRegression(penalty='l2', C=C, n_jobs=-1, random_state=seed)\n",
    "    \n",
    "    logit.fit(X_train, y_train)\n",
    "    \n",
    "    valid_pred = logit.predict_proba(X_valid)[:, 1]\n",
    "    \n",
    "    return round(roc_auc_score(y_valid, valid_pred), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_submission_file(predicted_labels, out_file,\n",
    "                             target='target', index_label=\"session_id\"):\n",
    "    predicted_df = pd.DataFrame(predicted_labels,\n",
    "                                index = np.arange(1, predicted_labels.shape[0] + 1),\n",
    "                                columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "times = [\"time%s\" % i for i in range(1, 11)]\n",
    "sites = [\"site%s\" % i for i in range(1, 11)]\n",
    "\n",
    "with open(PATH_TO_DATA + SITE_DIC, \"rb\") as inp_file:\n",
    "    site_dic = pickle.load(inp_file)\n",
    "\n",
    "inv_site_dic = {v: k for k, v in site_dic.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(PATH_TO_DATA + INP_TRAIN, \n",
    "                       index_col=\"session_id\", \n",
    "                       parse_dates=times).sort_values(by=\"time1\")\n",
    "train_df[sites] = train_df[sites].fillna(0).astype(\"int\")\n",
    "\n",
    "test_df = pd.read_csv(PATH_TO_DATA + INP_TEST,\n",
    "                       index_col=\"session_id\", \n",
    "                       parse_dates=times)\n",
    "test_df[sites] = test_df[sites].fillna(0).astype(\"int\")\n",
    "\n",
    "y_train = train_df[\"target\"]\n",
    "train_df.drop('target', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_to_text = train_df[sites].apply(\n",
    "    lambda x: \" \".join([str(a) for a in x.values if a != 0]), axis=1)\\\n",
    "               .values.reshape(len(train_df[sites]), 1)\n",
    "test_to_text = test_df[sites].apply(\n",
    "    lambda x: \" \".join([str(a) for a in x.values if a != 0]), axis=1)\\\n",
    "               .values.reshape(len(test_df[sites]), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((253561, 41592), (82797, 41592))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    (\"vectorize\", CountVectorizer()),\n",
    "    (\"tfidf\", TfidfTransformer())\n",
    "])\n",
    "pipeline.fit(train_to_text.ravel())\n",
    "\n",
    "X_train_sparse = pipeline.transform(train_to_text.ravel())\n",
    "X_test_sparse = pipeline.transform(test_to_text.ravel())\n",
    "\n",
    "X_train_sparse.shape, X_test_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_auc(X_train_sparse, y_train):\n",
    "    Cs = np.logspace(-3, 1, 10)\n",
    "    Ms = []\n",
    "    for C in Cs:\n",
    "        auc = get_auc_lr_valid(X_train_sparse, y_train, C=C)\n",
    "        Ms.append(auc)\n",
    "\n",
    "    for i, m in enumerate(Ms):\n",
    "        s = \"\"\n",
    "        if m == max(Ms): s += \"--> \"\n",
    "        s += \"C: %s, auc: %s\" % (Cs[i], m)\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_train = pd.DataFrame(index=train_df.index)\n",
    "feat_test = pd.DataFrame(index=test_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "lmbd = lambda ts: 100 * ts.year + ts.month\n",
    "feat_train['year_month'] = train_df['time1'].apply(lmbd)\n",
    "feat_test['year_month'] = test_df['time1'].apply(lmbd)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "feat_train['year_month_scaled'] = scaler.fit_transform(feat_train['year_month'].values.reshape(-1, 1))\n",
    "feat_test['year_month_scaled'] = scaler.transform(feat_test['year_month'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "lmbd = lambda ts: ts.year\n",
    "feat_train['year'] = train_df['time1'].apply(lmbd)\n",
    "feat_test['year'] = test_df['time1'].apply(lmbd)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "feat_train['year_scaled'] = scaler.fit_transform(feat_train['year'].values.reshape(-1, 1))\n",
    "feat_test['year_scaled'] = scaler.transform(feat_test['year'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "lmbd = lambda ts: ts.month\n",
    "feat_train['month'] = train_df['time1'].apply(lmbd)\n",
    "feat_test['month'] = test_df['time1'].apply(lmbd)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "feat_train['month_scaled'] = scaler.fit_transform(feat_train['month'].values.reshape(-1, 1))\n",
    "feat_test['month_scaled'] = scaler.transform(feat_test['month'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "lmbd = lambda ts: ts.hour\n",
    "feat_train['start_hour'] = train_df['time1'].apply(lmbd)\n",
    "feat_test['start_hour'] = test_df['time1'].apply(lmbd)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "feat_train['start_hour_scaled'] = scaler.fit_transform(feat_train['start_hour'].values.reshape(-1, 1))\n",
    "feat_test['start_hour_scaled'] = scaler.transform(feat_test['start_hour'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "lmbd = lambda ts: ts.dayofweek\n",
    "feat_train['weekday'] = train_df['time1'].apply(lmbd)\n",
    "feat_test['weekday'] = test_df['time1'].apply(lmbd)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "feat_train['weekday_scaled'] = scaler.fit_transform(feat_train['weekday'].values.reshape(-1, 1))\n",
    "feat_test['weekday_scaled'] = scaler.transform(feat_test['weekday'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmbd = lambda ts: int(ts.hour > 4 and ts.hour <= 11)\n",
    "feat_train['morning'] = train_df['time1'].apply(lmbd)\n",
    "feat_test['morning'] = test_df['time1'].apply(lmbd)\n",
    "\n",
    "lmbd = lambda ts: int(ts.hour > 11 and ts.hour <= 18)\n",
    "feat_train['work'] = train_df['time1'].apply(lmbd)\n",
    "feat_test['work'] = test_df['time1'].apply(lmbd)\n",
    "\n",
    "lmbd = lambda ts: int(ts.hour > 18 and ts.hour <= 23)\n",
    "feat_train['eve'] = train_df['time1'].apply(lmbd)\n",
    "feat_test['eve'] = test_df['time1'].apply(lmbd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "lmbd = lambda x: len(set(a for a in x.values if a != 0))\n",
    "feat_train['uniq_sites'] = train_df[sites].apply(lmbd, axis=1).values.reshape(len(train_df[sites]), 1)\n",
    "feat_test['uniq_sites'] = test_df[sites].apply(lmbd, axis=1).values.reshape(len(test_df[sites]), 1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "feat_train['uniq_sites_scaled'] = scaler.fit_transform(feat_train['uniq_sites'].values.reshape(-1, 1))\n",
    "feat_test['uniq_sites_scaled'] = scaler.transform(feat_test['uniq_sites'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmbd = lambda ts: max(ts).timestamp() - min(ts).timestamp()\n",
    "feat_train['session_timespan'] = train_df[times].apply(lmbd, axis=1).values.reshape(len(train_df[sites]), 1)\n",
    "feat_test['session_timespan'] = test_df[times].apply(lmbd, axis=1).values.reshape(len(test_df[sites]), 1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "feat_train['session_timespan_scaled'] = scaler.fit_transform(feat_train['session_timespan'].values.reshape(-1, 1))\n",
    "feat_test['session_timespan_scaled'] = scaler.transform(feat_test['session_timespan'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_features(subset):\n",
    "    X_train_sparse_new, X_test_sparse_new = X_train_sparse, X_test_sparse\n",
    "    \n",
    "    for feature in subset:\n",
    "        X_train_sparse_new = csr_matrix(hstack([X_train_sparse_new, \n",
    "                                        feat_train[feature].values.reshape(-1, 1)]))\n",
    "        X_test_sparse_new = csr_matrix(hstack([X_test_sparse_new, \n",
    "                                               feat_test[feature].values.reshape(-1, 1)]))\n",
    "        \n",
    "    return X_train_sparse_new, X_test_sparse_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "stuff = ['year_month', 'year_month_scaled', 'start_hour', 'start_hour_scaled',\n",
    "       'weekday', 'weekday_scaled', 'morning', 'uniq_sites',\n",
    "       'uniq_sites_scaled']\n",
    "auc = {}\n",
    "for L in range(1, len(stuff)+1):\n",
    "    for subset in itertools.combinations(stuff, L):\n",
    "        skip = False\n",
    "        for feature in subset:\n",
    "            found = sum(1 for s in subset if feature in s)\n",
    "            if found > 1:\n",
    "                skip = True\n",
    "        \n",
    "        if skip:\n",
    "            print(\"- skipped\", subset)\n",
    "            continue\n",
    "\n",
    "        X_train_sparse_new, X_test_sparse_new = add_features(subset)\n",
    "        r = get_auc_lr_valid(X_train_sparse_new, y_train, C=0.5)\n",
    "        auc[subset] = r\n",
    "        print(subset, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = sorted(auc.items(), key=lambda x:-x[1])[:20]\n",
    "for x in t:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = ('year_month_scaled', 'start_hour_scaled', 'weekday_scaled', 'morning', 'uniq_sites')\n",
    "X_train_sparse_new, X_test_sparse_new = add_features(subset)\n",
    "\n",
    "for C in np.logspace(-3, 2, 20):\n",
    "    print(C, get_auc_lr_valid(X_train_sparse_new, y_train, C=C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in [5, 10, 20, 30]:\n",
    "    in_top = \"in_top_\" + str(n)\n",
    "    in_top_scaled = in_top + \"_scaled\"\n",
    "    top_n_sites = train_df[y_train == 1][sites].stack().value_counts().nlargest(n).index\n",
    "\n",
    "    lmbd = lambda x: sum(1 for s in x.values if s != 0 and s in top_n_sites)\n",
    "    feat_train[in_top] = train_df[sites].apply(lmbd, axis=1).values.reshape(len(train_df[sites]), 1)\n",
    "    feat_test[in_top] = test_df[sites].apply(lmbd, axis=1).values.reshape(len(test_df[sites]), 1)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    feat_train[in_top_scaled] = scaler.fit_transform(feat_train[in_top].values.reshape(-1, 1))\n",
    "    feat_test[in_top_scaled] = scaler.transform(feat_test[in_top].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "stuff = ['in_top_5', 'in_top_5_scaled', \n",
    "         'in_top_10', 'in_top_10_scaled', 'in_top_20', 'in_top_20_scaled', 'in_top_30', 'in_top_30_scaled']\n",
    "subset = ('year_month_scaled', 'start_hour_scaled', 'weekday_scaled', 'morning', 'uniq_sites')\n",
    "\n",
    "auc1 = {}\n",
    "for f in stuff:\n",
    "    subset1 = subset + (f, )\n",
    "    X_train_sparse_new, X_test_sparse_new = add_features(subset1)\n",
    "    r = get_auc_lr_valid(X_train_sparse_new, y_train, C=0.5)\n",
    "    auc1[subset1] = r\n",
    "    print(subset1, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "logit = LogisticRegression(C=3.6, n_jobs=-1, random_state=17)\n",
    "logit.fit(X_train_sparse_top, y_train)\n",
    "y_pred = logit.predict_proba(X_test_sparse_top)[:, 1]\n",
    "\n",
    "write_to_submission_file(y_pred, PATH_TO_DATA + \"/submit/tfidf_yms_shs_ws_m_uniq_top30_c3_6.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "stuff = [\"session_timespan\", \"session_timespan_scaled\"]\n",
    "subset = ('year_month_scaled', 'start_hour_scaled', 'weekday_scaled', 'morning', 'uniq_sites_scaled')\n",
    "\n",
    "auc1 = {}\n",
    "for f in stuff:\n",
    "    subset1 = subset + (f, )\n",
    "    X_train_sparse_new, X_test_sparse_new = add_features(subset1)\n",
    "    r = get_auc_lr_valid(X_train_sparse_new, y_train, C=0.5)\n",
    "    auc1[subset1] = r\n",
    "    print(subset1, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "\n",
    "def svc_param_selection(X, y):\n",
    "    parameters = {'kernel':('linear',), 'C':[0.01, 0.1, 1]}\n",
    "    svc = svm.SVC()\n",
    "    grid_search = GridSearchCV(svc, parameters, n_jobs=-1, verbose=10)\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    return grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 0.88176\n",
      "0.00183298071083 0.94706\n",
      "0.00335981828628 0.95509\n",
      "0.00615848211066 0.95257\n",
      "0.0112883789168 0.95157\n",
      "0.0206913808111 0.95206\n",
      "0.0379269019073 0.95282\n",
      "0.0695192796178 0.95317\n",
      "0.12742749857 0.95311\n",
      "0.233572146909 0.95274\n",
      "0.428133239872 0.95215\n",
      "0.784759970351 0.95131\n",
      "1.43844988829 0.95018\n",
      "2.63665089873 0.94904\n",
      "4.83293023857 0.94802\n",
      "8.8586679041 0.94721\n",
      "16.2377673919 0.94647\n",
      "29.7635144163 0.9455\n",
      "54.5559478117 0.94423\n",
      "100.0 0.94266\n"
     ]
    }
   ],
   "source": [
    "subset = ('year_month_scaled', 'start_hour_scaled', 'weekday_scaled', 'morning', 'uniq_sites_scaled', 'session_timespan_scaled')\n",
    "X_train_sparse_new, X_test_sparse_new = add_features(subset)\n",
    "\n",
    "for C in np.logspace(-3, 2, 20):\n",
    "    print(C, get_auc_lr_valid(X_train_sparse_new, y_train, C=C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i1.ytimg.com\n",
      "s.youtube.com\n",
      "www.youtube.com\n",
      "www.facebook.com\n",
      "www.google.fr\n",
      "r4---sn-gxo5uxg-jqbe.googlevideo.com\n",
      "r1---sn-gxo5uxg-jqbe.googlevideo.com\n",
      "apis.google.com\n",
      "s.ytimg.com\n",
      "r2---sn-gxo5uxg-jqbe.googlevideo.com\n"
     ]
    }
   ],
   "source": [
    "top_n_sites = train_df[y_train == 1][sites].stack().value_counts().nlargest(10).index\n",
    "for s in top_n_sites:\n",
    "    print(inv_site_dic[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for n in [5, 10, 20, 30]:\n",
    "    in_top = \"in_top_\" + str(n)\n",
    "    top_n_sites = train_df[y_train == 1][sites].stack().value_counts().nlargest(n).index\n",
    "    \n",
    "    def lmbd(x):\n",
    "        for s in x.values: \n",
    "            if s == 0: continue \n",
    "            if s in top_n_sites: return 1\n",
    "        return 0\n",
    "\n",
    "    feat_train[in_top] = train_df[sites].apply(lmbd, axis=1).values.reshape(len(train_df[sites]), 1)\n",
    "    feat_test[in_top] = test_df[sites].apply(lmbd, axis=1).values.reshape(len(test_df[sites]), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('year_month_scaled', 'start_hour_scaled', 'weekday_scaled', 'morning', 'uniq_sites_scaled', 'session_timespan_scaled', 'in_top_5') 0.95283\n",
      "('year_month_scaled', 'start_hour_scaled', 'weekday_scaled', 'morning', 'uniq_sites_scaled', 'session_timespan_scaled', 'in_top_10') 0.95301\n",
      "('year_month_scaled', 'start_hour_scaled', 'weekday_scaled', 'morning', 'uniq_sites_scaled', 'session_timespan_scaled', 'in_top_20') 0.95287\n",
      "('year_month_scaled', 'start_hour_scaled', 'weekday_scaled', 'morning', 'uniq_sites_scaled', 'session_timespan_scaled', 'in_top_30') 0.95213\n"
     ]
    }
   ],
   "source": [
    "stuff = ['in_top_5', 'in_top_10', 'in_top_20', 'in_top_30']\n",
    "subset = ('year_month_scaled', 'start_hour_scaled', 'weekday_scaled', 'morning', 'uniq_sites_scaled', 'session_timespan_scaled')\n",
    "\n",
    "auc1 = {}\n",
    "for f in stuff:\n",
    "    subset1 = subset + (f, )\n",
    "    X_train_sparse_new, X_test_sparse_new = add_features(subset1)\n",
    "    r = get_auc_lr_valid(X_train_sparse_new, y_train, C=0.1)\n",
    "    auc1[subset1] = r\n",
    "    print(subset1, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lmbd = lambda ts: max(ts).timestamp() - min(ts).timestamp()\n",
    "feat_train['session_timespan'] = train_df[times].apply(lmbd, axis=1).values.reshape(len(train_df[sites]), 1)\n",
    "feat_test['session_timespan'] = test_df[times].apply(lmbd, axis=1).values.reshape(len(test_df[sites]), 1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "feat_train['session_timespan_scaled'] = scaler.fit_transform(feat_train['session_timespan'].values.reshape(-1, 1))\n",
    "feat_test['session_timespan_scaled'] = scaler.transform(feat_test['session_timespan'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_time_diff = np.diff(train_df[times], axis=1)/np.timedelta64(1, 's')\n",
    "test_time_diff = np.diff(test_df[times], axis=1)/np.timedelta64(1, 's')\n",
    "deltas = [\"delta%s\" % str(i) for i in range(1, 10)]\n",
    "for i, delta in enumerate(deltas):\n",
    "    feat_train[delta] = train_time_diff[:, i]\n",
    "    feat_train[delta] = feat_train[delta].fillna(0)\n",
    "    \n",
    "    feat_test[delta] = test_time_diff[:, i]\n",
    "    feat_test[delta] = feat_test[delta].fillna(0)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    feat_train[delta + '_scaled'] = scaler.fit_transform(feat_train[delta].values.reshape(-1, 1))\n",
    "    feat_test[delta + '_scaled'] = scaler.transform(feat_test[delta].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year_month</th>\n",
       "      <th>year_month_scaled</th>\n",
       "      <th>start_hour</th>\n",
       "      <th>start_hour_scaled</th>\n",
       "      <th>weekday</th>\n",
       "      <th>weekday_scaled</th>\n",
       "      <th>morning</th>\n",
       "      <th>uniq_sites</th>\n",
       "      <th>uniq_sites_scaled</th>\n",
       "      <th>session_timespan</th>\n",
       "      <th>...</th>\n",
       "      <th>delta9</th>\n",
       "      <th>delta1_scaled</th>\n",
       "      <th>delta2_scaled</th>\n",
       "      <th>delta3_scaled</th>\n",
       "      <th>delta4_scaled</th>\n",
       "      <th>delta5_scaled</th>\n",
       "      <th>delta6_scaled</th>\n",
       "      <th>delta7_scaled</th>\n",
       "      <th>delta8_scaled</th>\n",
       "      <th>delta9_scaled</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201410</td>\n",
       "      <td>0.822948</td>\n",
       "      <td>11</td>\n",
       "      <td>-0.407823</td>\n",
       "      <td>5</td>\n",
       "      <td>1.682905</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1.747312</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.191229</td>\n",
       "      <td>-0.170968</td>\n",
       "      <td>-0.186631</td>\n",
       "      <td>-0.193139</td>\n",
       "      <td>-0.192809</td>\n",
       "      <td>-0.193165</td>\n",
       "      <td>-0.192025</td>\n",
       "      <td>-0.104099</td>\n",
       "      <td>-0.190791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201407</td>\n",
       "      <td>0.752287</td>\n",
       "      <td>11</td>\n",
       "      <td>-0.407823</td>\n",
       "      <td>3</td>\n",
       "      <td>0.441028</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.858194</td>\n",
       "      <td>85.0</td>\n",
       "      <td>...</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.061235</td>\n",
       "      <td>-0.133105</td>\n",
       "      <td>-0.100195</td>\n",
       "      <td>-0.154907</td>\n",
       "      <td>-0.179620</td>\n",
       "      <td>-0.013667</td>\n",
       "      <td>-0.105958</td>\n",
       "      <td>-0.176180</td>\n",
       "      <td>0.150664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201412</td>\n",
       "      <td>0.870055</td>\n",
       "      <td>15</td>\n",
       "      <td>0.858234</td>\n",
       "      <td>4</td>\n",
       "      <td>1.061966</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.457582</td>\n",
       "      <td>84.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.181131</td>\n",
       "      <td>-0.170968</td>\n",
       "      <td>0.472445</td>\n",
       "      <td>-0.180395</td>\n",
       "      <td>-0.179620</td>\n",
       "      <td>-0.179357</td>\n",
       "      <td>-0.177680</td>\n",
       "      <td>0.011230</td>\n",
       "      <td>-0.146253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201411</td>\n",
       "      <td>0.846501</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.724338</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.800850</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.545477</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.191229</td>\n",
       "      <td>-0.170968</td>\n",
       "      <td>-0.175826</td>\n",
       "      <td>-0.193139</td>\n",
       "      <td>-0.179620</td>\n",
       "      <td>-0.193165</td>\n",
       "      <td>-0.192025</td>\n",
       "      <td>-0.176180</td>\n",
       "      <td>-0.190791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>201405</td>\n",
       "      <td>0.705179</td>\n",
       "      <td>15</td>\n",
       "      <td>0.858234</td>\n",
       "      <td>4</td>\n",
       "      <td>1.061966</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.946088</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.181131</td>\n",
       "      <td>-0.170968</td>\n",
       "      <td>-0.121804</td>\n",
       "      <td>-0.180395</td>\n",
       "      <td>-0.192809</td>\n",
       "      <td>-0.193165</td>\n",
       "      <td>-0.192025</td>\n",
       "      <td>-0.190596</td>\n",
       "      <td>-0.131407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            year_month  year_month_scaled  start_hour  start_hour_scaled  \\\n",
       "session_id                                                                 \n",
       "1               201410           0.822948          11          -0.407823   \n",
       "2               201407           0.752287          11          -0.407823   \n",
       "3               201412           0.870055          15           0.858234   \n",
       "4               201411           0.846501          10          -0.724338   \n",
       "5               201405           0.705179          15           0.858234   \n",
       "\n",
       "            weekday  weekday_scaled  morning  uniq_sites  uniq_sites_scaled  \\\n",
       "session_id                                                                    \n",
       "1                 5        1.682905        1          10           1.747312   \n",
       "2                 3        0.441028        1           1          -1.858194   \n",
       "3                 4        1.061966        0           2          -1.457582   \n",
       "4                 1       -0.800850        1           7           0.545477   \n",
       "5                 4        1.061966        0           8           0.946088   \n",
       "\n",
       "            session_timespan      ...        delta9  delta1_scaled  \\\n",
       "session_id                        ...                                \n",
       "1                        7.0      ...           0.0      -0.191229   \n",
       "2                       85.0      ...          23.0       0.061235   \n",
       "3                       84.0      ...           3.0      -0.181131   \n",
       "4                        4.0      ...           0.0      -0.191229   \n",
       "5                       13.0      ...           4.0      -0.181131   \n",
       "\n",
       "            delta2_scaled  delta3_scaled  delta4_scaled  delta5_scaled  \\\n",
       "session_id                                                               \n",
       "1               -0.170968      -0.186631      -0.193139      -0.192809   \n",
       "2               -0.133105      -0.100195      -0.154907      -0.179620   \n",
       "3               -0.170968       0.472445      -0.180395      -0.179620   \n",
       "4               -0.170968      -0.175826      -0.193139      -0.179620   \n",
       "5               -0.170968      -0.121804      -0.180395      -0.192809   \n",
       "\n",
       "            delta6_scaled  delta7_scaled  delta8_scaled  delta9_scaled  \n",
       "session_id                                                              \n",
       "1               -0.193165      -0.192025      -0.104099      -0.190791  \n",
       "2               -0.013667      -0.105958      -0.176180       0.150664  \n",
       "3               -0.179357      -0.177680       0.011230      -0.146253  \n",
       "4               -0.193165      -0.192025      -0.176180      -0.190791  \n",
       "5               -0.193165      -0.192025      -0.190596      -0.131407  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((253561, 41609), (82797, 41609))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = ('year_month_scaled', 'start_hour_scaled', 'weekday_scaled', 'morning', 'work', 'eve',\n",
    "          'uniq_sites', 'in_top_10')\n",
    "subset += tuple([\"delta%s_scaled\" % str(i) for i in range(1, 10)]) \n",
    "X_train_sparse_new, X_test_sparse_new = add_features(subset)\n",
    "\n",
    "X_train_sparse_new.shape, X_test_sparse_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 0.5438\n",
      "0.01 0.91725\n",
      "0.1 0.95233\n",
      "1 0.95259\n",
      "10 0.95104\n",
      "100 0.94867\n",
      "1000 0.9412\n"
     ]
    }
   ],
   "source": [
    "for C in [0.001, 0.01, 0.1, 1, 10, 100, 1000]:\n",
    "    print(C, get_auc_lr_valid(X_train_sparse_new, y_train, C=C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.51 s, sys: 0 ns, total: 8.51 s\n",
      "Wall time: 8.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "logit = LogisticRegression(penalty=\"l2\", C=1, n_jobs=-1, random_state=17)\n",
    "logit.fit(X_train_sparse_new, y_train)\n",
    "y_pred = logit.predict_proba(X_test_sparse_new)[:, 1]\n",
    "\n",
    "write_to_submission_file(y_pred, PATH_TO_DATA + \"/submit/tfidf_deltas.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
